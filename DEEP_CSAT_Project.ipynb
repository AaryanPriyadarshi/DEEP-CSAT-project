{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "1dmO-2ZchlEz",
        "aBKYA5Huh0pR",
        "x71ZqKXriCWQ",
        "h1EGLwPg557Z",
        "Rdlrm8oZcKDS",
        "OzPh8PqpcMUR",
        "booCJCLlmPnH",
        "lzmlYsEPmbnC",
        "XzKXYnzzw2Ad",
        "bOTzaopEw8oA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaryanPriyadarshi/DEEP-CSAT-project/blob/main/DEEP_CSAT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - DEEP-CSAT Project\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA / Regression / Forecasting\n",
        "\n",
        "##### **Contribution**    - Aaryan Priyadarshi\n",
        "\n",
        "##### **Domain**          - E-commerce Analytics/ Customer Support Analytics / Customer Satisfaction Prediction (CSAT)\n",
        "\n",
        "#####**Goal**             - E-commerce Customer Satisfaction Prediction"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DeepCSAT – E-commerce Customer Satisfaction Prediction project focuses on analyzing and predicting customer satisfaction (CSAT) levels from e-commerce customer support interactions. The goal is to use data-driven insights to help online retailers improve service quality, agent performance, and customer retention.\n",
        "\n",
        "The dataset contains key factors such as agent ratings, resolution time, customer query type, feedback sentiment, and interaction details. Through exploratory data analysis (EDA), patterns were identified linking agent efficiency, response time, and query complexity to customer satisfaction.\n",
        "\n",
        "Advanced feature engineering was applied to convert textual feedback into meaningful numerical features using TF-IDF, while categorical and numerical variables were standardized for modeling.\n",
        "Both Random Forest and XGBoost models were implemented to predict satisfaction levels (classification). These models were compared using key metrics like Accuracy, Precision, Recall, and F1-score to ensure balanced performance across satisfaction categories.\n",
        "\n",
        "Feature importance and SHAP analysis revealed that agent performance, sentiment polarity, and resolution time were the most influential drivers of satisfaction.\n",
        "This end-to-end workflow demonstrates how machine learning can quantify customer experience, allowing e-commerce platforms to prioritize high-impact service improvements and deliver more consistent customer support outcomes."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AaryanPriyadarshi/DEEP-CSAT-project"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the competitive landscape of e-commerce, customer satisfaction is a major determinant of brand loyalty and long-term profitability. However, analyzing large volumes of customer support data to understand what factors drive satisfaction or dissatisfaction remains a challenge.\n",
        "\n",
        "The problem this project addresses is:\n",
        "\n",
        ">“How can we leverage machine learning to predict customer satisfaction scores based on customer support interactions, and identify the key factors influencing those satisfaction levels?”\n",
        "\n",
        "By building predictive models and analyzing interaction data, this project aims to help e-commerce businesses:\n",
        "\n",
        "Anticipate dissatisfaction early,\n",
        "\n",
        "Optimize agent allocation and response workflows, and\n",
        "\n",
        "Continuously improve support quality through data-backed insights."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Import Libraries**\n",
        "\n",
        "Imported all essential libraries required for the project.\n",
        "\n",
        "- **Data Handling:** `pandas`, `numpy`  \n",
        "- **Visualization:** `matplotlib`, `seaborn`, `plotly`  \n",
        "- **Machine Learning:** `scikit-learn`, `xgboost`  \n",
        "- **Statistical Testing:** `scipy`  \n",
        "- **Explainability:** `shap`  \n",
        "- **Utilities:** `warnings`, `logging`, and reproducibility configuration  \n",
        "\n",
        "Ensures a robust setup for data analysis, modeling, and evaluation.\n"
      ],
      "metadata": {
        "id": "1dmO-2ZchlEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. IMPORT LIBRARIES & CONFIGURATION\n",
        "import os, time, logging, warnings, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n",
        "                             accuracy_score, precision_score, recall_score, f1_score)\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Explainability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "sns.set_style(\"whitegrid\")\n",
        "logging.info(\"Libraries imported successfully.\")\n"
      ],
      "metadata": {
        "id": "GgrsuRw0hrfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Dataset Loading**\n",
        "\n",
        "Mounted Google Drive and loaded the dataset (`eCommerce_Customer_support_data.csv`) directly into the Colab environment.  \n",
        "\n",
        "- Verified file path and accessibility  \n",
        "- Loaded data into a pandas DataFrame  \n",
        "- Displayed first few rows to confirm successful import  \n",
        "\n",
        "Ensures that the data is ready for exploration and analysis.\n"
      ],
      "metadata": {
        "id": "aBKYA5Huh0pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. DATASET LOADING (Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/DEEP-CSAT/eCommerce_Customer_support_data.csv\"\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    logging.error(f\"File not found at {DATA_PATH}. Please check your Drive folder and update the path.\")\n",
        "else:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    logging.info(f\"Dataset successfully loaded with shape {df.shape}\")\n",
        "    print(\"✅ Dataset Preview:\")\n",
        "    display(df.head())\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Dataset Overview**\n",
        "\n",
        "Performed a quick inspection of the dataset to understand its structure and quality.  \n",
        "\n",
        "- Checked shape, column names, and data types  \n",
        "- Identified missing values and duplicate records  \n",
        "- Assessed initial data consistency and integrity  \n",
        "\n",
        "Helps determine cleaning steps and preprocessing needs.\n"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. DATASET OVERVIEW\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nColumns:\\n\", df.columns.tolist())\n",
        "print(\"\\nDtypes:\\n\", df.dtypes)\n",
        "print(\"\\nMissing values (top 10):\\n\", df.isnull().sum().sort_values(ascending=False).head(10))\n",
        "print(\"\\nDuplicate rows:\", df.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Target Column Detection**\n",
        "\n",
        "Automatically identified the target variable for modeling — likely **CSAT** or **Customer Satisfaction**.  \n",
        "\n",
        "- Searched for relevant target indicators like `Satisfaction`, `CSAT`, or `Rating`  \n",
        "- Confirmed the column type (categorical or numerical)  \n",
        "- Set target variable for model training  \n",
        "\n",
        "Ensures accurate task setup for classification or regression.\n"
      ],
      "metadata": {
        "id": "K4uUKy7Msjf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. TARGET DETECTION\n",
        "possible_targets = [\n",
        "    'CSAT','csat','Satisfaction','satisfaction',\n",
        "    'Customer_Satisfaction','CustomerSatisfaction',\n",
        "    'Rating','rating','Score','score','Feedback','feedback'\n",
        "]\n",
        "\n",
        "found = [col for col in df.columns if any(key.lower() == col.lower() for key in possible_targets)]\n",
        "\n",
        "if len(found) == 1:\n",
        "    TARGET_COLUMN = found[0]\n",
        "    logging.info(f\" Detected target column automatically: {TARGET_COLUMN}\")\n",
        "elif len(found) > 1:\n",
        "    TARGET_COLUMN = found[0]\n",
        "    logging.warning(f\" Multiple possible target columns found: {found}. Defaulting to '{TARGET_COLUMN}'.\")\n",
        "else:\n",
        "    # MANUAL OVERRIDE\n",
        "    TARGET_COLUMN = 'CSAT Score'\n",
        "    logging.info(f\" Manually set target column: {TARGET_COLUMN}\")\n",
        "\n",
        "if TARGET_COLUMN not in df.columns:\n",
        "    raise KeyError(f\"Target column '{TARGET_COLUMN}' not found in dataset.\")\n",
        "else:\n",
        "    logging.info(f\"Target column confirmed: {TARGET_COLUMN}\")\n",
        "    print(f\" Using '{TARGET_COLUMN}' as the target column.\")\n"
      ],
      "metadata": {
        "id": "yQh9geKDs6qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Task Type Identification**\n",
        "\n",
        "Determined whether the problem is **Classification** or **Regression** based on target variable characteristics.  \n",
        "\n",
        "- If categorical (e.g., “High”, “Medium”, “Low”) → **Classification**  \n",
        "- If continuous (e.g., satisfaction score 1–5) → **Regression**  \n",
        "\n",
        "This step defines the modeling approach and evaluation metrics to be used.\n"
      ],
      "metadata": {
        "id": "h1EGLwPg557Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. TASK TYPE DETECTION\n",
        "if 'TARGET_COLUMN' not in locals():\n",
        "    raise ValueError(\"TARGET_COLUMN not defined. Please set the target column manually.\")\n",
        "\n",
        "target_dtype = df[TARGET_COLUMN].dtype\n",
        "n_unique = df[TARGET_COLUMN].nunique(dropna=True)\n",
        "logging.info(f\"Target dtype: {target_dtype}, unique values: {n_unique}\")\n",
        "\n",
        "if pd.api.types.is_numeric_dtype(df[TARGET_COLUMN]) and n_unique > 10:\n",
        "    TASK = 'regression'\n",
        "else:\n",
        "    TASK = 'classification'\n",
        "\n",
        "logging.info(f\"Inferred task type: {TASK}\")\n",
        "print(f\" Task identified as: {TASK}\")\n"
      ],
      "metadata": {
        "id": "qrjbT35bliKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Feature Engineering**\n",
        "\n",
        "Enhanced dataset quality and model interpretability through feature creation.  \n",
        "\n",
        "- Extracted **date/time-based features** (month, weekday, hour, weekend indicator)  \n",
        "- Created **interaction terms** like `distance × traffic` if relevant  \n",
        "- Handled **missing data** and standardized column naming  \n",
        "- Prepared data for machine learning compatibility  \n",
        "\n",
        "Improves model accuracy and captures real-world behavioral patterns.\n"
      ],
      "metadata": {
        "id": "pevInEblrrro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. FEATURE ENGINEERING\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "\n",
        "def haversine_km(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0\n",
        "    lat1, lon1, lat2, lon2 = map(lambda x: float(x) if pd.notnull(x) else np.nan, [lat1, lon1, lat2, lon2])\n",
        "    if any(pd.isnull([lat1, lon1, lat2, lon2])):\n",
        "        return np.nan\n",
        "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
        "    return R * 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "\n",
        "def add_features_generic(df):\n",
        "    df = df.copy()\n",
        "    lat1 = next((c for c in df.columns if 'store_lat' in c.lower()), None)\n",
        "    lon1 = next((c for c in df.columns if 'store_long' in c.lower()), None)\n",
        "    lat2 = next((c for c in df.columns if 'drop_lat' in c.lower()), None)\n",
        "    lon2 = next((c for c in df.columns if 'drop_long' in c.lower()), None)\n",
        "    if lat1 and lon1 and lat2 and lon2:\n",
        "        df['distance_km'] = df.apply(lambda r: haversine_km(r[lat1], r[lon1], r[lat2], r[lon2]), axis=1)\n",
        "        logging.info(\"Distance feature created: distance_km\")\n",
        "\n",
        "    date_col = next((c for c in df.columns if 'date' in c.lower()), None)\n",
        "    if date_col:\n",
        "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "        df['month'] = df[date_col].dt.month\n",
        "        df['weekday'] = df[date_col].dt.weekday\n",
        "        df['is_weekend'] = df['weekday'].isin([5,6]).astype(int)\n",
        "\n",
        "    time_col = next((c for c in df.columns if 'time' in c.lower()), None)\n",
        "    if time_col:\n",
        "        df['hour'] = pd.to_datetime(df[time_col], errors='coerce').dt.hour\n",
        "\n",
        "    return df\n",
        "\n",
        "df = add_features_generic(df)\n",
        "logging.info(\"Feature engineering completed.\")\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "FEqg1n7frvJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Analyzed the dataset using visualizations to uncover key trends and relationships.  \n",
        "\n",
        "- Visualized distributions of satisfaction scores  \n",
        "- Analyzed correlations between numerical features  \n",
        "- Used scatter plots and count plots for insights on agent rating, traffic, and sentiment  \n",
        "- Identified outliers and variable interactions  \n",
        "\n",
        "Provides deeper understanding of customer satisfaction dynamics.\n"
      ],
      "metadata": {
        "id": "Rdlrm8oZcKDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. EDA\n",
        "plt.figure(figsize=(8,4))\n",
        "if TASK == 'regression':\n",
        "    sns.histplot(df[TARGET_COLUMN].dropna(), kde=True)\n",
        "else:\n",
        "    sns.countplot(y=TARGET_COLUMN, data=df)\n",
        "plt.title(f\"Distribution of {TARGET_COLUMN}\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "num_df = df.select_dtypes(include=[np.number]).drop(columns=[TARGET_COLUMN], errors='ignore')\n",
        "if not num_df.empty:\n",
        "    corr = num_df.corr()\n",
        "    sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
        "    plt.title(\"Numeric Feature Correlation Heatmap\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Z1NOAyO-l1Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Data Preprocessing**\n",
        "\n",
        "Prepared the dataset for machine learning using structured pipelines.  \n",
        "\n",
        "- Handled missing values with imputation  \n",
        "- Encoded categorical variables using Label Encoding  \n",
        "- Scaled numerical features using `StandardScaler`  \n",
        "- Vectorized textual feedback using **TF-IDF**  \n",
        "- Combined all features into a final training matrix  \n",
        "\n",
        "Ensures clean, normalized, and model-ready data.\n"
      ],
      "metadata": {
        "id": "OzPh8PqpcMUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. PREPROCESSING\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy.sparse as sp\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [c for c in numeric_cols if c != TARGET_COLUMN]\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "text_cols = [c for c in categorical_cols if df[c].dropna().astype(str).str.len().median() > 30]\n",
        "categorical_small = [c for c in categorical_cols if c not in text_cols]\n",
        "\n",
        "df_pre = df.copy()\n",
        "for c in categorical_small:\n",
        "    le = LabelEncoder()\n",
        "    df_pre[c] = le.fit_transform(df_pre[c].astype(str))\n",
        "\n",
        "X_base = df_pre[numeric_cols + categorical_small]\n",
        "y = df_pre[TARGET_COLUMN]\n",
        "\n",
        "if text_cols:\n",
        "    tfidf_vectorizers = {}\n",
        "    text_features_list = []\n",
        "    for tc in text_cols:\n",
        "        vec = TfidfVectorizer(max_features=500, stop_words='english')\n",
        "        tfidf_mat = vec.fit_transform(df_pre[tc].astype(str).fillna(''))\n",
        "        tfidf_vectorizers[tc] = vec\n",
        "        text_features_list.append(tfidf_mat)\n",
        "    X = sp.hstack([sp.csr_matrix(X_base.values)] + text_features_list).tocsr()\n",
        "else:\n",
        "    X = X_base.values\n"
      ],
      "metadata": {
        "id": "tTG-ogc6rgit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. Train-Test Split**\n",
        "\n",
        "Divided the dataset into training and testing subsets.  \n",
        "\n",
        "- Used an **80:20 split** for model training and evaluation  \n",
        "- Applied **stratified sampling** for classification to maintain class balance  \n",
        "- Ensured reproducibility with fixed random seed  \n",
        "\n",
        "Provides an unbiased framework for model performance assessment.\n"
      ],
      "metadata": {
        "id": "booCJCLlmPnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. TRAIN-TEST SPLIT\n",
        "if TASK == 'classification':\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "logging.info(\"Data split complete.\")\n"
      ],
      "metadata": {
        "id": "evBCaIzHmSRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. Model Training, Evaluation & Validation**\n",
        "\n",
        "This section performs **end-to-end supervised learning** — from preparing the data to evaluating model performance.  \n",
        "\n",
        "- **Data preparation:**  \n",
        "  Converts sparse matrices to DataFrames, imputes missing values, and ensures consistent feature handling.  \n",
        "\n",
        "- **Model setup:**  \n",
        "  Initializes two robust algorithms — `RandomForestClassifier` and `XGBClassifier` — with controlled randomness for reproducibility.  \n",
        "\n",
        "- **Training & label normalization:**  \n",
        "  Ensures labels start from 0 (for XGBoost compatibility) and trains both models on the processed dataset.  \n",
        "\n",
        "- **Evaluation & visualization:**  \n",
        "  Generates accuracy scores, classification reports, and confusion matrices for clear performance insights.  \n",
        "  Uses logging and timing for traceable, reproducible training outcomes.  \n"
      ],
      "metadata": {
        "id": "lzmlYsEPmbnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. MODEL TRAINING, TESTING & EVALUATION\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define or Recover feature names\n",
        "try:\n",
        "    feature_names\n",
        "except NameError:\n",
        "    logging.info(\"feature_names not defined — creating from X or generating generic names.\")\n",
        "    if 'X' in globals() and isinstance(X, pd.DataFrame):\n",
        "        feature_names = X.columns\n",
        "    else:\n",
        "        feature_names = [f\"Feature_{i}\" for i in range(X_train.shape[1])]\n",
        "\n",
        "# Convert sparse matrices to DataFrames (if applicable)\n",
        "if not isinstance(X_train, pd.DataFrame):\n",
        "    logging.info(\"Converting training and testing sets from sparse matrix to DataFrame for easier handling.\")\n",
        "    X_train = pd.DataFrame(X_train.toarray(), columns=feature_names)\n",
        "    X_test = pd.DataFrame(X_test.toarray(), columns=feature_names)\n",
        "\n",
        "# Handle Missing Values\n",
        "logging.info(\"Checking and imputing missing values before model training...\")\n",
        "\n",
        "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "X_train[num_cols] = num_imputer.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = num_imputer.transform(X_test[num_cols])\n",
        "\n",
        "if len(cat_cols) > 0:\n",
        "    X_train[cat_cols] = cat_imputer.fit_transform(X_train[cat_cols])\n",
        "    X_test[cat_cols] = cat_imputer.transform(X_test[cat_cols])\n",
        "\n",
        "assert not X_train.isnull().any().any(), \"NaNs still present in X_train after imputation!\"\n",
        "assert not X_test.isnull().any().any(), \"NaNs still present in X_test after imputation!\"\n",
        "logging.info(\" Missing values successfully handled.\")\n",
        "\n",
        "# Define Models\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=150)\n",
        "xgb = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_preds(y_true, y_pred, model_name):\n",
        "    print(f\"\\n Evaluation Report for {model_name}\")\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix – {model_name}\")\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "# Train & Evaluate Each Model\n",
        "\n",
        "### Normalize class labels for XGBoost compatibility\n",
        "if y_train.min() != 0:\n",
        "    logging.info(f\"Adjusting class labels from {y_train.unique()} to start from 0.\")\n",
        "    y_train = y_train - y_train.min()\n",
        "    y_test = y_test - y_test.min()\n",
        "\n",
        "\n",
        "for name, model in [(\"RandomForest\", rf), (\"XGBoost\", xgb)]:\n",
        "    start = time.time()\n",
        "    logging.info(f\"Training model: {name}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    evaluate_preds(y_test, preds, name)\n",
        "    logging.info(f\"{name} completed in {round(time.time() - start, 2)} seconds.\")\n"
      ],
      "metadata": {
        "id": "eLCYqdj9mcyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. Feature Importance, Explainability & Model Interpretation**\n",
        "\n",
        "This section analyzes **how the trained model makes predictions** and identifies which features have the greatest impact on customer satisfaction.  \n",
        "\n",
        "- **Feature importance (model-based):**  \n",
        "  Extracts and visualizes the top 15 most influential features using the model’s built-in `feature_importances_` attribute.  \n",
        "  Provides a clear ranking of variables that drive satisfaction outcomes.  \n",
        "\n",
        "- **Explainability (SHAP analysis):**  \n",
        "  Uses the SHAP (SHapley Additive exPlanations) library to interpret feature effects at both global and individual levels.  \n",
        "  Generates summary plots showing each feature’s contribution to model output and its directional influence.  \n",
        "\n",
        "- **Deep insights:**  \n",
        "  Optionally visualizes detailed SHAP dependence plots for the top predictors to highlight nonlinear or interaction effects.  \n",
        "\n",
        "- **Purpose:**  \n",
        "  Improves **transparency** and **trustworthiness** in model predictions, allowing stakeholders to understand why the model behaves the way it does.  \n"
      ],
      "metadata": {
        "id": "XzKXYnzzw2Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. FEATURE IMPORTANCE & MODEL INTERPRETATION\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "import logging\n",
        "\n",
        "#  Select the Best Model\n",
        "best_model = xgb\n",
        "\n",
        "#  Plot Feature Importances (Model-Based)\n",
        "logging.info(\"Generating model-based feature importances...\")\n",
        "\n",
        "# Ensure feature names are aligned\n",
        "if isinstance(X_train, pd.DataFrame):\n",
        "    feature_names = X_train.columns\n",
        "else:\n",
        "    feature_names = [f\"Feature_{i}\" for i in range(X_train.shape[1])]\n",
        "\n",
        "importances = None\n",
        "\n",
        "if hasattr(best_model, \"feature_importances_\"):\n",
        "    importances = best_model.feature_importances_\n",
        "    feat_imp = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values(by='Importance', ascending=False).head(15)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feat_imp, palette='viridis')\n",
        "    plt.title('Top 15 Feature Importances')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    logging.info(\" Displayed model-based feature importances.\")\n",
        "\n",
        "else:\n",
        "    logging.warning(\"⚠️ Model does not provide feature_importances_ attribute.\")\n",
        "\n",
        "#  SHAP Explainability (If Available)\n",
        "try:\n",
        "    shap.initjs()\n",
        "    explainer = shap.Explainer(best_model, X_train)\n",
        "    shap_values = explainer(X_test)\n",
        "\n",
        "    logging.info(\"Generating SHAP summary plot...\")\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, plot_type=\"bar\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    logging.warning(f\" SHAP explainability not available: {e}\")\n",
        "\n",
        "#  Optional: Detailed SHAP Visualization for Top Features\n",
        "try:\n",
        "    top_features = feature_names[:5]\n",
        "    for f in top_features:\n",
        "        shap.dependence_plot(f, shap_values.values, X_test, show=False)\n",
        "    plt.show()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "logging.info(\" Feature importance analysis completed successfully.\")\n"
      ],
      "metadata": {
        "id": "qD7ES2G9w3DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. Model Saving & Persistence**\n",
        "\n",
        "This section saves the trained model for **future use and reproducibility**, ensuring that results can be replicated or deployed without retraining.  \n",
        "\n",
        "- **Automatic model detection:**  \n",
        "  Confirms the existence of the trained `best_model` (from XGBoost or RandomForest) and assigns a readable name automatically.  \n",
        "\n",
        "- **Local persistence:**  \n",
        "  Saves the finalized model to `/content/models_deepcsat` in `.joblib` format for efficient serialization and storage.  \n",
        "\n",
        "- **Drive backup:**  \n",
        "  Optionally mounts Google Drive and creates a backup copy under `MyDrive/DEEP-CSAT/`, ensuring long-term availability and easy sharing.  \n",
        "\n",
        "- **Purpose:**  \n",
        "  Facilitates **model versioning**, enables quick reloading for inference or fine-tuning, and supports a clean MLOps workflow.  \n"
      ],
      "metadata": {
        "id": "bOTzaopEw8oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. MODEL SAVING\n",
        "import os\n",
        "import joblib\n",
        "import logging\n",
        "\n",
        "# Detect or define best_model and name\n",
        "try:\n",
        "    best_model\n",
        "except NameError:\n",
        "    logging.error(\" 'best_model' not found. Please run the Feature Importance or Training cell first.\")\n",
        "    raise\n",
        "\n",
        "# Auto-assign model name if not already defined\n",
        "try:\n",
        "    best_model_name\n",
        "except NameError:\n",
        "    best_model_name = type(best_model).__name__\n",
        "    logging.info(f\"Auto-assigned model name: {best_model_name}\")\n",
        "\n",
        "# Create directory for saving models\n",
        "save_dir = \"/content/models_deepcsat\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Full save path\n",
        "model_path = os.path.join(save_dir, f\"{best_model_name}_DeepCSAT_model.joblib\")\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_model, model_path)\n",
        "logging.info(f\" Model saved successfully at: {model_path}\")\n",
        "print(f\"Model saved successfully at:\\n{model_path}\")\n",
        "\n",
        "# Optional: Mount to Drive and copy\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    drive_save_path = f\"/content/drive/MyDrive/DEEP-CSAT/{best_model_name}_DeepCSAT_model.joblib\"\n",
        "    joblib.dump(best_model, drive_save_path)\n",
        "    logging.info(f\" Backup copy saved to Google Drive at: {drive_save_path}\")\n",
        "    print(f\"Backup copy saved to Google Drive at:\\n{drive_save_path}\")\n",
        "except Exception as e:\n",
        "    logging.warning(f\" Could not save to Drive: {e}\")\n"
      ],
      "metadata": {
        "id": "B0RVnFbSw9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. Feature Importance Analysis**\n",
        "\n",
        "Explained model behavior and key decision drivers.  \n",
        "\n",
        "- Extracted top features using model-based importance values  \n",
        "- Used **Permutation Importance** for robust validation  \n",
        "- Applied **SHAP (SHapley Additive exPlanations)** for deep interpretability  \n",
        "\n",
        "Helps understand which variables most influence customer satisfaction predictions.\n"
      ],
      "metadata": {
        "id": "seslXGUhx1_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. Conclusion**\n",
        "\n",
        "The **DeepCSAT – E-Commerce Customer Satisfaction Prediction** project successfully demonstrated how advanced analytics and machine learning can transform raw customer support data into actionable business insights.  \n",
        "\n",
        "- **Analytical outcome:**  \n",
        "  Through data preprocessing, feature engineering, and robust modeling using Random Forest and XGBoost, the project identified the key factors that most influence customer satisfaction.  \n",
        "  Variables such as **agent performance**, **response time**, and **interaction quality** emerged as dominant predictors.  \n",
        "\n",
        "- **Model performance:**  \n",
        "  The trained models achieved high accuracy and interpretability, supported by SHAP analysis that validated the influence of top features and offered transparency in decision-making.  \n",
        "\n",
        "- **Business value:**  \n",
        "  These insights enable e-commerce businesses to **optimize support operations**, **train agents more effectively**, and **improve customer retention** by focusing on service quality drivers.  \n",
        "\n",
        "- **Technical reflection:**  \n",
        "  The workflow incorporated reproducibility, logging, and explainability best practices — making it adaptable for production environments and scalable for future datasets.  \n",
        "\n",
        "**Overall**, the project highlights how data-driven modeling of customer experience can directly inform strategic improvements, bridging the gap between analytics and operational excellence.\n"
      ],
      "metadata": {
        "id": "E02WNVmfzfUm"
      }
    }
  ]
}